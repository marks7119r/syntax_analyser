Lexical and Syntax Analyzer

This project implements the first two phases of a compiler: Lexical Analysis and Syntax Analysis. The code is written in Python and demonstrates the core concepts of tokenization (Lexical Analysis) and parsing (Syntax Analysis) for a simple high-level static-typed programming language.

Table of Contents
* Introduction
* Installation
* Usage
* Modules
* Error Handling
* License

Introduction
This project is focused on the Lexical Analyzer and Syntax Analyzer components of a compiler.

Lexical Analyzer:
The Lexical Analyzer (or lexer) scans the input source code and converts it into a sequence of tokens. Tokens represent the smallest meaningful elements in the source code (e.g., keywords, operators, identifiers, and numbers).

Syntax Analyzer:
The Syntax Analyzer (or parser) takes the list of tokens generated by the Lexical Analyzer and checks if the sequence follows the grammar rules of the language. It generates an Abstract Syntax Tree (AST) that represents the syntactic structure of the input.

Installation
To run this project, ensure you have Python 3.x installed on your machine. You can check your Python version by:

bash
Copy code
python --version
If Python is installed, clone this repository and navigate to the project folder:

bash
Copy code
git clone https://github.com/yourusername/compiler-lexical-syntax.git
cd compiler-lexical-syntax
This project does not have any external dependencies, so no installation of packages is necessary.

Usage
To use the Lexical and Syntax Analyzer, you can call the tokenize() and parse() functions with a string of source code.
Or directly use the SyntaxAnalyzerGUI.py file to run the GUI-enabled version

Example Usage

# Sample input string (source code)
source_code = "int x = 5 + 3 * (2 - 1);"

# Tokenize the input string
tokens = tokenize(source_code)
print("Tokens:", tokens)

# Parse the tokens into a Parse trree (AST)
ast = parse(tokens)
print("Abstract Syntax Tree:", ast)
Output:
python
Copy code
Tokens: [('DATATYPE', 'int'), ('IDENTIFIER', 'x'), ('ASSIGN', '='), ('NUMBER', '5'), ('OPERATOR', '+'), ('NUMBER', '3'), ('OPERATOR', '*'), ('LPAREN', '('), ('NUMBER', '2'), ('OPERATOR', '-'), ('NUMBER', '1'), ('RPAREN', ')'), ('EOF', '')]
Abstract Syntax Tree: {'type': 'Assignment', 'left': {'type': 'Identifier', 'name': 'x'}, 'right': {'type': 'BinaryOperation', 'operator': '=', 'left': {'type': 'BinaryOperation', 'operator': '+', 'left': {'type': 'Number', 'value': '5'}, 'right': {'type': 'BinaryOperation', 'operator': '*', 'left': {'type': 'Number', 'value': '3'}, 'right': {'type': 'BinaryOperation', 'operator': '-', 'left': {'type': 'Number', 'value': '2'}, 'right': {'type': 'Number', 'value': '1'}}}}}}

Modules
tokenize(input_string)
This function takes an input string and breaks it into a series of tokens using regular expressions. It identifies key components such as keywords, operators, identifiers, and numbers.

Parameters:

input_string (str): The source code to tokenize.
Returns:

A list of tuples, where each tuple contains a token type and its corresponding value.
Raises:

SyntaxError: If an illegal character is encountered.
parse(tokens)
This function takes a list of tokens and processes them into an Abstract Syntax Tree (AST), ensuring that the sequence of tokens follows the correct syntax according to the grammar rules.

Parameters:

tokens (list): A list of token tuples to be parsed.
Returns:

A dictionary representing the root node of the Abstract Syntax Tree (AST).
Raises:

SyntaxError: If the tokens do not form a valid syntactic structure.
parse_assignment(tokens)
This function processes an assignment statement, where a variable is assigned a value. It checks if the first token is a datatype, followed by an identifier, an assignment operator (=), and an expression on the right-hand side.

Parameters:

tokens (list): A list of tokens representing an assignment statement.
Returns:

A dictionary representing the assignment in the AST.
parse_expression(tokens)
This function handles parsing of expressions involving binary operators such as +, -, or %. It combines terms into a larger expression.

Parameters:

tokens (list): A list of tokens representing an expression.
Returns:

A dictionary representing the root node of the parsed expression.
parse_term(tokens)
This function parses terms connected by multiplication (*) or division (/) operators. It recursively processes factors in the term.

Parameters:

tokens (list): A list of tokens representing terms.
Returns:

A dictionary representing the parsed term.
parse_factor(tokens)
This function processes factors, such as numbers, identifiers, unary operators (-, !, ~), and expressions inside parentheses.

Parameters:

tokens (list): A list of tokens representing factors in an expression.
Returns:

A dictionary representing the parsed factor.
Error Handling
During the tokenization and parsing processes, if the input does not match the expected structure, a SyntaxError will be raised.

For example:

If an invalid character is encountered during tokenization.
If the parser expects an identifier or an operator but encounters an unexpected token.
Example:

python
Copy code
tokenize("int x = + 5")
# Raises SyntaxError: Illegal character at position 7


